# -*- coding: utf-8 -*-
import os
import json
import asyncio
import aiohttp
import time
import re
import uuid
from contextlib import asynccontextmanager

try:
    from google import genai
    from google.genai import types

    HAS_GOOGLE_GENAI = True
except Exception:
    HAS_GOOGLE_GENAI = False

# ==========================================
# 1. åŸºç¡€è·¯å¾„é…ç½®
# ==========================================
CONFIG_FILENAME = "3.ç»¼è¿°èƒŒæ™¯.json"
PACK_FILE_PATH = os.path.join("2.1åˆæˆpackæ–‡ä»¶", "pack.md")
DIR_OUTPUT_BG = "3.1ç»¼è¿°èƒŒæ™¯"

# ==========================================
# 2. åŠ¨æ€æç¤ºè¯æ¨¡æ¿
# ==========================================
PROMPT_TEMPLATE = """
Step 1 ç»¼è¿°èƒŒæ™¯ç”Ÿæˆæ¨¡æ¿
ä½ æ˜¯é¡¶çº§æœŸåˆŠé£æ ¼çš„ç»¼è¿°å†™ä½œé¡¾é—®ã€‚è¯·åŸºäºæˆ‘æä¾›çš„å­é¢†åŸŸ Pack å†…å®¹ï¼Œç”Ÿæˆâ€œæè¿°-è¯„ä»·-å¼•é¢˜â€ä¸‰æ®µå¼èƒŒæ™¯æ–‡æœ¬ã€‚

ã€è¾“å…¥è®¾å®šã€‘
åˆ‡å…¥é¢†åŸŸï¼š{subfield_name}
ç›®æ ‡å­—æ•°ï¼š{target_words}å­—å·¦å³

ã€ç¡¬çº¦æŸã€‘
1. åªä½¿ç”¨ Pack ä¸­â€œæ ¸å¿ƒä»·å€¼æ€»ç»“ + å‚è€ƒæ–‡çŒ®æ¡ç›®â€çš„ä¿¡æ¯ã€‚
2. ä¸é€ç¯‡ç½—åˆ—ï¼Œå¿…é¡»åšæ¦‚å¿µèšåˆã€‚
3. ä¸è®¨è®ºç ”ç©¶ç¼ºå£ä¸åˆ›æ–°æ€§ï¼ˆç•™ç»™åç»­æ­¥éª¤ï¼‰ã€‚
4. åœ¨å¼•ç”¨çš„è¯­å¥ä¸­ä½¿ç”¨ï¼ˆAuthor,yearï¼‰[n]ï¼Œå…¶ä¸­[n]ä¸ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„å¼•ç”¨æ¡ç›®çš„åºå·ã€‚ä¾‹å¦‚ï¼š(Su & Cheng, 2023)[2]ã€‚
5. æ’ç‰ˆè¦æ±‚ï¼šä¸¥æ ¼ä½¿ç”¨ Markdown æ ¼å¼ï¼Œä¸»æ ‡é¢˜ä½¿ç”¨ ###ã€‚

ã€è¾“å‡ºç»“æ„ã€‘ï¼ˆä¸¥æ ¼æŒ‰åºï¼‰

### 1) æè¿°ï¼ˆDescriptionï¼‰
ç•Œå®šè¯¥è®®é¢˜åœ¨å­¦ç§‘ä¸­çš„ç ”ç©¶é‡è¦æ€§
æç‚¼ 3-5 ä¸ªé«˜é¢‘æ¦‚å¿µ/ç†è®ºæ ‡ç­¾

### 2) è¯„ä»·ï¼ˆEvaluationï¼‰
å½’çº³ä¸»è¦ç ”ç©¶æ–¹æ³•ä¸è¯æ®ç»“æ„
æ€»ç»“å·²æœ‰ç ”ç©¶çš„è§£é‡Šè´¡çŒ®ä¸è¾¹ç•Œ

### 3) å¼•é¢˜ï¼ˆLead-inï¼‰
æ”¶æ•›åˆ°ä¸€ä¸ªå¯ç»§ç»­å±•å¼€çš„è§£é‡Šè§†è§’
ä½œä¸ºåç»­æ­¥éª¤çš„è‡ªç„¶è¿‡æ¸¡ï¼Œä¸æå‰å†™ Gap

### 4) å‚è€ƒæ–‡çŒ®
ä»…åˆ—æ­£æ–‡å¼•ç”¨åˆ°çš„æ–‡çŒ®ï¼ˆGBT7714-2015ï¼‰ã€‚
**æ’ç‰ˆå¼ºçº¦æŸ**ï¼šè¯·åŠ¡å¿…ä»¥ [1], [2], [3] çš„ç¼–å·å½¢å¼ç‹¬ç«‹æˆè¡Œè¾“å‡ºï¼Œæ¯ä¸€è¡Œåªå†™ä¸€æ¡å‚è€ƒæ–‡çŒ®ï¼Œç»å¯¹ä¸è¦æŠŠå¤šæ¡æ–‡çŒ®è¿åœ¨ä¸€è¡Œï¼
æŒ‰ç…§ GBT7714-2015 é¡ºåºåˆ—å‡ºï¼Œæ¯ä¸€æ¡å‚è€ƒç‹¬ç«‹æˆè¡Œï¼Œæ¯ä¸€è¡Œåé¢åŠ ä¸Šä¸€ä¸ªå¯ä»¥è¢«mdæ–‡ä»¶è¯†åˆ«çš„æ¢è¡Œç¬¦ï¼Œä¸¥ç¦è¿å†™ã€‚
"""


# ==========================================
# [æ–°å¢] ä»£ç çº§å¼ºåˆ¶æ¸…æ´—ä¸æ’ç‰ˆå‡½æ•°
# ==========================================
def beautify_step1_markdown(text):
    if not text:
        return text

    # 1. å®šä½åˆ°â€œ### 4) å‚è€ƒæ–‡çŒ®â€è¿™ä¸€èŠ‚
    # å°†æ–‡æœ¬ä¸€åˆ†ä¸ºäºŒï¼šå‰é¢çš„æ­£æ–‡éƒ¨åˆ†ï¼Œå’Œåé¢çš„å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
    match = re.search(r'(### 4\)\s*å‚è€ƒæ–‡çŒ®.*?\n)(.*)', text, flags=re.IGNORECASE | re.DOTALL)

    if match:
        before_refs = text[:match.start(2)]
        refs_content = match.group(2)

        # 2. å¼ºè¡Œåˆ‡å‰²å‚è€ƒæ–‡çŒ®ï¼šæ‰¾åˆ°æ‰€æœ‰ç±»ä¼¼ "[1]", "[2]" çš„æ ‡å·ï¼Œå¹¶åœ¨å®ƒä»¬å‰é¢åŠ æ¢è¡Œç¬¦
        # æ­£åˆ™å«ä¹‰ï¼šåŒ¹é…ä»»æ„ç©ºç™½ç¬¦ + [æ•°å­—] + ä»»æ„ç©ºç™½ç¬¦ï¼Œæ›¿æ¢ä¸º "\n[æ•°å­—] "
        cleaned_refs = re.sub(r'\s*\[(\d+)\]\s*', r'\n[\1] ', refs_content)

        # å»é™¤å¯èƒ½äº§ç”Ÿçš„å¤šä½™è¿ç»­ç©ºè¡Œ
        cleaned_refs = re.sub(r'\n{2,}', r'\n', cleaned_refs).strip()

        # 3. é‡æ–°æ‹¼æ¥æ–‡æœ¬
        text = before_refs + cleaned_refs + '\n'

    # 4. é¡ºä¾¿æŠŠæ‰€æœ‰ä¸»æ ‡é¢˜å‰å¼ºåˆ¶åŠ ç©ºè¡Œï¼Œè®©æ’ç‰ˆæ›´é€æ°”
    text = re.sub(r'(?<!\n)\n(### \d\))', r'\n\n\1', text)

    return text.strip()


def log(msg, color="white"):
    timestamp = time.strftime("%H:%M:%S", time.localtime())
    colors = {"green": "\033[92m", "cyan": "\033[96m", "yellow": "\033[93m", "red": "\033[91m"}
    print(f"{colors.get(color, '')}[{timestamp}] {msg}\033[0m")


# ==========================================
# 3. æç®€ç‰ˆ API è°ƒç”¨å™¨
# ==========================================
async def call_gemini(pack_content, prompt, config, key_index=None):
    gcfg = config.get("Google_Native_Config", {})
    keys = gcfg.get("api_keys", [])
    model_name = gcfg.get("model_name", "gemini-2.5-flash")

    if not keys or not HAS_GOOGLE_GENAI:
        raise RuntimeError("Google Gemini é…ç½®ç¼ºå¤±æˆ–æœªå®‰è£…SDKã€‚")

    # âœ… å¹¶å‘æ¨¡å¼ï¼šå›ºå®šä¸€ä¸ª keyï¼›é¡ºåºæ¨¡å¼ï¼šä»æŒ‰ keys é€ä¸ªå°è¯•ï¼ˆåŸé€»è¾‘ï¼‰
    if key_index is not None:
        keys_to_try = [keys[key_index % len(keys)]]
    else:
        keys_to_try = keys

    for attempt, key in enumerate(keys_to_try, 1):
        try:
            client = genai.Client(api_key=key)
            cfg = types.GenerateContentConfig(temperature=0.3, system_instruction=prompt)
            log(f"   -> [Gemini] æ­£åœ¨æ€è€ƒä¸­ (ä½¿ç”¨ Key {attempt}/{len(keys_to_try)})...", "cyan")

            resp = await asyncio.to_thread(
                client.models.generate_content,
                model=model_name,
                contents=[f"ä»¥ä¸‹æ˜¯å­é¢†åŸŸæ–‡çŒ® Pack çš„å†…å®¹ï¼š\n\n{pack_content}"],
                config=cfg
            )
            if resp.text: return resp.text
        except Exception as e:
            log(f"âš ï¸ Gemini Key {attempt} å¤±è´¥: {e}", "yellow")
            await asyncio.sleep(2)

    raise RuntimeError("æ‰€æœ‰ Gemini Key å‡è°ƒç”¨å¤±è´¥ã€‚")


async def call_openai(pack_content, prompt, config, node_index=None):
    ocfg = config.get("OpenAI_Protocol_Config", {})
    pool = ocfg.get("api_pool", [])
    proxy = config.get("Settings", {}).get("proxy_url", None)

    if not pool: raise RuntimeError("OpenAI èŠ‚ç‚¹æ± ä¸ºç©ºã€‚")

    timeout = aiohttp.ClientTimeout(total=180)

    # âœ… å¹¶å‘æ¨¡å¼ï¼šå…ˆå›ºå®šä¸»èŠ‚ç‚¹ï¼›è‹¥ä¸»èŠ‚ç‚¹å‘ç”Ÿå¼‚å¸¸ï¼Œå†è‡ªåŠ¨åˆ‡åˆ°å¤‡ç”¨èŠ‚ç‚¹æ± 
    if node_index is not None:
        primary_idx = node_index % len(pool)
        primary_node = pool[primary_idx]
        backup_nodes = [node for idx, node in enumerate(pool) if idx != primary_idx]
        nodes_to_try = [primary_node]
    else:
        backup_nodes = []
        nodes_to_try = pool

    async with aiohttp.ClientSession(timeout=timeout) as session:
        switched_to_backup = False
        attempt = 0
        while nodes_to_try:
            node = nodes_to_try.pop(0)
            attempt += 1
            try:
                remark = node.get("remark", f"Node-{attempt}")
                url = f"{node['base_url'].rstrip('/')}{node['api_path']}"
                headers = {"Authorization": f"Bearer {node['api_key']}", "Content-Type": "application/json"}
                payload = {
                    "model": node["model_name"],
                    "messages": [
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯å­é¢†åŸŸæ–‡çŒ® Pack çš„å†…å®¹ï¼š\n\n{pack_content}"}
                    ],
                    "temperature": 0.3
                }

                log(f"   -> [OpenAI] æ­£åœ¨æ€è€ƒä¸­ (è°ƒç”¨èŠ‚ç‚¹: {remark})...", "cyan")
                async with session.post(url, headers=headers, json=payload, proxy=proxy) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return data["choices"][0]["message"]["content"]
                    else:
                        txt = await resp.text()
                        log(f"âš ï¸ OpenAI èŠ‚ç‚¹ {remark} å¤±è´¥ {resp.status}: {txt[:100]}", "yellow")
            except Exception as e:
                log(f"âš ï¸ OpenAI èŠ‚ç‚¹å¼‚å¸¸: {e}", "yellow")
                # ä»…åœ¨å¹¶å‘æ¨¡å¼ä¸‹ï¼šä¸»èŠ‚ç‚¹å‡ºç°å¼‚å¸¸æ—¶ï¼Œè‡ªåŠ¨å¯ç”¨å¤‡ç”¨èŠ‚ç‚¹æ± 
                if node_index is not None and not switched_to_backup and backup_nodes:
                    nodes_to_try.extend(backup_nodes)
                    switched_to_backup = True
                    log(f"âš ï¸ ä¸»èŠ‚ç‚¹å¼‚å¸¸ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨èŠ‚ç‚¹ï¼ˆå…± {len(backup_nodes)} ä¸ªï¼‰ã€‚", "yellow")

            await asyncio.sleep(2)

    raise RuntimeError("æ‰€æœ‰ OpenAI èŠ‚ç‚¹å‡è°ƒç”¨å¤±è´¥ã€‚")


# ==========================================
# 4. ä¸»ç¨‹åºæµç¨‹
# ==========================================
async def main():
    print("===========================================")
    print("         ç»¼è¿°èƒŒæ™¯ç”Ÿæˆå™¨ (Step 1)           ")
    print("===========================================")

    os.makedirs(DIR_OUTPUT_BG, exist_ok=True)

    try:
        with open(CONFIG_FILENAME, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        log(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ '{CONFIG_FILENAME}'", "red")
        return

    if not os.path.exists(PACK_FILE_PATH):
        log(f"âŒ æ‰¾ä¸åˆ° Pack æ–‡ä»¶ '{PACK_FILE_PATH}'ï¼Œè¯·å…ˆè¿è¡Œåˆæˆè„šæœ¬ã€‚", "red")
        return

    with open(PACK_FILE_PATH, 'r', encoding='utf-8') as f:
        pack_content = f.read()

    log(f"âœ… æˆåŠŸè¯»å– Pack æ–‡ä»¶ (å­—ç¬¦æ•°: {len(pack_content)})", "green")

    print("\n-------------------------------------------")
    subfield_name = input("âœï¸  è¯·è¾“å…¥ã€åˆ‡å…¥é¢†åŸŸã€‘(ä¾‹å¦‚: ä¼ä¸šæ•°å­—åŒ–è½¬å‹ä¸åˆ›æ–°ç»©æ•ˆ): ").strip()
    target_words = input("âœï¸  è¯·è¾“å…¥ã€ç›®æ ‡å­—æ•°ã€‘(ä¾‹å¦‚: 1500): ").strip()
    run_times = input("âœï¸  è¯·è¾“å…¥ã€è¿è¡Œæ¬¡æ•°ã€‘(ä¾‹å¦‚: 3): ").strip()
    use_concurrency_in = input("âš¡ æ˜¯å¦å¯ç”¨ã€å¹¶å‘æ¨¡å¼ã€‘? (y/N): ").strip().lower()
    use_concurrency = use_concurrency_in in ("y", "yes", "1", "true", "t")

    if not subfield_name or not target_words or not run_times:
        log("âŒ é¢†åŸŸ/å­—æ•°/è¿è¡Œæ¬¡æ•°ä¸èƒ½ä¸ºç©ºï¼Œç¨‹åºé€€å‡ºã€‚", "red")
        return

    try:
        run_times = int(run_times)
        if run_times <= 0:
            raise ValueError
    except Exception:
        log("âŒ è¿è¡Œæ¬¡æ•°å¿…é¡»æ˜¯æ­£æ•´æ•°ã€‚", "red")
        return

    final_prompt = PROMPT_TEMPLATE.format(
        subfield_name=subfield_name,
        target_words=target_words
    )

    provider = config.get("Settings", {}).get("interface_type", "openai_protocol")

    # âœ… å…³é”®è§„åˆ™ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹/Keyï¼Œåˆ™å¼ºåˆ¶é¡ºåºï¼ˆé¿å…å•èŠ‚ç‚¹å¹¶å‘ï¼‰
    openai_pool = config.get("OpenAI_Protocol_Config", {}).get("api_pool", [])
    gemini_keys = config.get("Google_Native_Config", {}).get("api_keys", [])

    capacity = len(openai_pool) if provider == "openai_protocol" else len(gemini_keys)
    if capacity < 2:
        if use_concurrency:
            log("âš ï¸ æ£€æµ‹åˆ°å¯ç”¨èŠ‚ç‚¹/Key å°‘äº 2 ä¸ªï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸ºã€é¡ºåºæ¨¡å¼ã€‘ä»¥é¿å…å•èŠ‚ç‚¹å¹¶å‘ã€‚", "yellow")
        use_concurrency = False

    log("\nğŸš€ å¼€å§‹è°ƒåº¦ AI å¤§æ¨¡å‹ç”Ÿæˆç»¼è¿°èƒŒæ™¯...", "cyan")
    log(f"æ¨¡å¼ï¼š{'å¹¶å‘' if use_concurrency else 'é¡ºåº'}ï¼›è½®æ¬¡ï¼š{run_times}", "cyan")

    safe_name = subfield_name.replace("/", "_").replace("\\", "_")

    async def run_one(i: int):
        log(f"ğŸ” ç¬¬ {i}/{run_times} æ¬¡ç”Ÿæˆä¸­...", "cyan")

        if provider == "native_response":
            result_md = await call_gemini(
                pack_content, final_prompt, config,
                key_index=(i - 1) if use_concurrency else None
            )
        elif provider == "openai_protocol":
            result_md = await call_openai(
                pack_content, final_prompt, config,
                node_index=(i - 1) if use_concurrency else None
            )
        else:
            raise ValueError(f"æœªçŸ¥çš„åè®®ç±»å‹: {provider}")

        if not result_md:
            raise RuntimeError("AI è¿”å›äº†ç©ºç»“æœã€‚")

        # ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ã€‘ä½¿ç”¨åº•å±‚ä»£ç è¿›è¡Œæ´—ç¨¿æ’ç‰ˆ
        result_md = beautify_step1_markdown(result_md)

        # âœ… ä¸è¦†ç›–å‘½åï¼šrunåºå· + æ—¶é—´æˆ³ + çŸ­UUIDï¼ˆå¹¶å‘åŒç§’ä¹Ÿä¸ä¼šæ’ï¼‰
        ts = time.strftime("%Y%m%d_%H%M%S", time.localtime())
        uid = uuid.uuid4().hex[:8]
        output_path = os.path.join(
            DIR_OUTPUT_BG,
            f"Step1_{safe_name}_èƒŒæ™¯_run{i:02d}_{ts}_{uid}.md"
        )

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"# ç»¼è¿°èƒŒæ™¯ï¼š{subfield_name}\n\n")
            f.write(result_md)

        log(f"âœ… ç¬¬ {i} æ¬¡å®Œæˆï¼š{output_path}", "green")
        return output_path

    outputs = []
    if use_concurrency:
        tasks = [asyncio.create_task(run_one(i)) for i in range(1, run_times + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for i, r in enumerate(results, 1):
            if isinstance(r, Exception):
                log(f"âŒ ç¬¬ {i} æ¬¡å¤±è´¥: {r}", "red")
            else:
                outputs.append(r)
    else:
        for i in range(1, run_times + 1):
            try:
                outputs.append(await run_one(i))
            except Exception as e:
                log(f"âŒ ç¬¬ {i} æ¬¡å¤±è´¥: {e}", "red")
                continue

    log(f"\nğŸ‰ ç»“æŸï¼šæˆåŠŸ {len(outputs)}/{run_times} æ¬¡ã€‚", "green")


if __name__ == "__main__":
    import sys

    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
